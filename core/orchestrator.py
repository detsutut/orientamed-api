from typing import Any, Literal
from boto3 import Session
from typing_extensions import List, TypedDict
import textwrap
import json
import requests
import pandas as pd
import logging

from langchain_aws import InMemoryVectorStore
from langchain_core.messages import BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END
from langgraph.types import Command
from langchain_core.messages.human import HumanMessage
import os
import yaml

from core.kg_retriever import KGRetriever
from core.languagemodel import LanguageModel
from core.retriever import Retriever
from core.data_models import RetrievedDocument, LLMResponse, References, Concepts, Concept, ConsumedTokens, \
    RerankedDocument, LLMResponseStatus
from core.reranker import RRFReranker, TopKReranker

logger = logging.getLogger('app.' + __name__)
logging.getLogger("langchain_aws").setLevel(logging.WARNING)
logging.getLogger("langchain_core").setLevel(logging.WARNING)

with open(os.getenv("CORE_SETTINGS_PATH")) as stream:
    rag_config = yaml.safe_load(stream)


def messages_to_history_str(messages: list[BaseMessage]) -> str:
    """Convert messages to a history string."""
    string_messages = []
    for message in messages:
        role = message.type
        content = message.content
        string_message = f"{role}: {content}"
        additional_kwargs = message.additional_kwargs
        if additional_kwargs:
            string_message += f"\n{additional_kwargs}"
        string_messages.append(string_message)
    return "\n".join(string_messages)


class Prompts:
    def __init__(self, jsonfile: str):
        with open(jsonfile, 'r') as file:
            data = json.load(file)
            for k, v in data.items():
                self.__setattr__(k, self.__parse__(v))

    def __parse__(self, prompt_dicts: List[dict]):
        return ChatPromptTemplate([(d["role"], d["content"]) for d in prompt_dicts])


# Define state for application
from typing_extensions import Annotated
from operator import add


class State(TypedDict):
    # INPUTS
    query: str # the user query
    history: List[BaseMessage]  # all the interactions between ai and user
    additional_context: str  # additional info added by the user to be considered a valid source
    query_aug: bool  # use or not query augmentation technique before passing the query to the retriever
    use_graph: bool
    use_embeddings: bool
    retrieve_only: bool
    pre_translate: bool
    reranker: str  # RRF or topk
    max_refs: int # max reference to use to answer
    check_consistency: bool
    # INTERNAL
    consolidated_query: str
    docs_reranked: list[RetrievedDocument] # redundant but convenient for internal processing
    # OUTPUTS
    status: LLMResponseStatus
    input_tokens_count: Annotated[
        int, add]  # input tokens processed by the whole chain of llm calls triggered in this round
    output_tokens_count: Annotated[
        int, add]  # output tokens processed by the whole chain of llm calls triggered in this round
    answer: str  # textual answer generated by the system and returned to the user
    query_concepts: List[Concept] # list of concepts extracted from input query
    answer_concepts: List[Concept] # list of concepts extracted from generated answer
    docs_graph: list[RetrievedDocument]  # retrieved documents from graph
    docs_embeddings: list[RetrievedDocument]  # retrieved documents from embeddings
    reranked_ids_and_scores: list[RerankedDocument] # only store ids and scores to reduce payload size since the rest can be already found in docs_graph and docs_embeddings
    references: list[RetrievedDocument]  # what has been actually used as reference


def __get_document_from_retrieved_list__(id:str, list: List[RetrievedDocument]):
    for doc in list:
        if doc.metadata.get("doc_id") == id:
            return doc
    return None


class Orchestrator:

    def __init__(self, session: Session,
                 vector_store: InMemoryVectorStore | str | None = None):
        client = session.client("bedrock-runtime", region_name=rag_config.get("bedrock").get("region"))
        self.prompts = Prompts(rag_config.get("promptfile"))
        self.llm = LanguageModel(client=client,
                                 model=rag_config.get("bedrock").get("models").get("model-id"),
                                 model_low=rag_config.get("bedrock").get("models").get("low-model-id", None),
                                 model_pro=rag_config.get("bedrock").get("models").get(" pro-model-id", None))
        self.retriever = Retriever(client=client,
                                   embedder=rag_config.get("bedrock").get("embedder-id"),
                                   vector_store=vector_store)
        self.retriever_kg = KGRetriever()
        self.retrieve_size = 20
        self.reranker = RRFReranker(k=15)  # 60 is too much for less than 50 chunks

        graph_builder = StateGraph(state_schema=State)
        graph_builder.set_entry_point("dispatcher")
        #NODES
        graph_builder.add_node("dispatcher", self.dispatcher)
        graph_builder.add_node("history_consolidator", self.history_consolidator)
        graph_builder.add_node("augmenter", self.augmenter)
        graph_builder.add_node("emb_retriever", self.emb_retriever)
        graph_builder.add_node("doc_reranker", self.doc_reranker)
        graph_builder.add_node("ans_generator", self.ans_generator)
        graph_builder.add_node("consistency_checker", self.consistency_checker)
        graph_builder.add_node("kg_retriever", self.kg_retriever)
        #EDGES
        graph_builder.add_edge("dispatcher", "history_consolidator")
        graph_builder.add_edge("history_consolidator", "augmenter")
        graph_builder.add_edge("augmenter", "emb_retriever")
        graph_builder.add_edge("augmenter", "kg_retriever")
        graph_builder.add_edge("emb_retriever", "doc_reranker")
        graph_builder.add_edge("kg_retriever", "doc_reranker")
        graph_builder.add_edge("doc_reranker", "ans_generator")
        self.graph = graph_builder.compile()

    def dispatcher(self, state: State) -> dict:
        logger.info(f"Dispatching request...")
        logger.debug(
            f"Request details:\n\t-Query: {textwrap.shorten(state['query'], 30)}\n\t-History: {len(state['history'])} messages\n\t-Additional context: {state['additional_context']}\n\t-Query augmentation: {state['query_aug']}\n\t-Graph DB: {state['use_graph']}\n\t-Vector DB: {state['use_embeddings']}\n\t-Retrieve only: {state['retrieve_only']}\n\t-Pre-translate: {state['pre_translate']}")
        return {}

    def history_consolidator(self, state: State) -> dict:
        previous_user_interactions = [message for message in state["history"] if type(message) is HumanMessage]
        if len(previous_user_interactions) > 0:
            logger.info(f"Consolidating history...")
            messages = self.prompts.history_consolidation.invoke({"question": state["query"],
                                                                  "history": messages_to_history_str(
                                                                      state["history"])}).messages
            response = self.llm.generate(messages=messages)
            consolidated_query = response.content
            logger.info(f"Consolidated query: {textwrap.shorten(consolidated_query, width=200)}")
            update = {"consolidated_query": consolidated_query,
                      "input_tokens_count": response.usage_metadata["input_tokens"],
                      "output_tokens_count": response.usage_metadata["output_tokens"]}
        else:
            logger.info(f"First interaction, history consolidation skipped.")
            update = {"consolidated_query": None,
                      "input_tokens_count": 0,
                      "output_tokens_count": 0}
        return update

    def augmenter(self, state: State) -> dict:
        update = {}
        if state["query_aug"]:
            user_query = state["consolidated_query"] if state["consolidated_query"] else state["query"]
            logger.info(f"Expanding Query...")
            messages = self.prompts.query_expansion.invoke({"question": user_query}).messages
            response = self.llm.generate(messages=messages)
            augmented_query = response.content
            logger.info(f"Expanded query: {textwrap.shorten(augmented_query, width=30)}")
            update = {"query": augmented_query,
                      "input_tokens_count": response.usage_metadata["input_tokens"],
                      "output_tokens_count": response.usage_metadata["output_tokens"]
                      }
        return update

    def __translate__(self, text: str) -> str:
        logger.debug(f"Translating Text in English before feeding to Concept Extractor...")
        messages = self.prompts.translation.invoke({"source_lang": "Italian",
                                                    "target_lang": "English",
                                                    "source_text": text,
                                                    }).messages
        response = self.llm.generate(messages=messages, level="pro")
        input_tokens = response.usage_metadata["input_tokens"]
        output_tokens = response.usage_metadata["output_tokens"]
        translated_text = response.content
        logger.debug(f"Translated test: {textwrap.shorten(translated_text, width=30)}")
        return translated_text, input_tokens, output_tokens

    def __concept_extraction__(self, text: str, min_overlap_perc=100, use_premium_translation=False, pre_translate=False) -> (List[Concept], int, int):
        if pre_translate:
            text, input_tokens, output_tokens = self.__translate__(text)
        else:
            input_tokens = 0
            output_tokens = 0
        logger.info(f"Extracting Concepts...")
        url = "https://dheal-com.unipv.it:7878/extract"
        params = {'text': text, 'o': min_overlap_perc, 'p': use_premium_translation}
        response = requests.get(url, params=params)
        concepts = []
        if response.status_code != 200:
            logger.error("Error during concept extraction. Further investigation needed.")
            logger.debug(f"Raw response: {response}")
        else:
            try:
                concepts = pd.DataFrame(response.json()).to_dict(orient='records')
            except Exception as e:
                logger.error(f"Error during concept extraction: {e}")
                logger.debug(f"Raw response: {response}")
        if len(concepts) == 0 and not use_premium_translation:
            logger.debug("No concepts found, trying with premium translation")
            params = {'text': text, 'o': 100, 'p': True}
            response = requests.get(url, params=params)
            concepts = pd.DataFrame(response.json()).to_dict(orient='records')
        concepts = [Concept(**concept) for concept in concepts]
        return concepts, input_tokens, output_tokens

    def kg_retriever(self, state: State) -> dict:
        if not state["use_graph"]:
            logger.debug(f"Graph not activated, bypassed.")
            update = {"docs_graph": [],
                      "query_concepts": []}
        else:
            # CONCEPT EXTRACTION
            concepts, input_tokens, output_tokens = self.__concept_extraction__(state["query"], pre_translate=state["pre_translate"])
            # DOC RETRIEVAL
            logger.info(f"Retrieving Nodes...")
            retrieved_docs = self.retriever_kg.retrieve_average_shortest([c.id for c in concepts], max_hops=5)[:self.retrieve_size]
            update = {"query_concepts": concepts,
                      "input_tokens_count": input_tokens,
                      "output_tokens_count": output_tokens,
                      "docs_graph": retrieved_docs
                      }
        return update

    def emb_retriever(self, state: State) -> dict:
        retrieved_docs = []
        input_tokens = 0
        output_tokens = 0
        if not state["use_embeddings"]:
            logger.debug(f"Embeddings not activated, bypassed.")
        else:
            logger.info(f"Retrieving Documents...")
            user_query = state["consolidated_query"] if state["consolidated_query"] else state["query"]
            try:
                retrieved_docs = self.retriever.retrieve_with_scores(user_query, n=self.retrieve_size, score_threshold=0.4)
            except Exception as e:
                logger.info(f"Error during retrieving documents: {e}")
                messages = self.prompts.summarization.invoke({"content": user_query}).messages
                response = self.llm.generate(messages=messages, level="pro")
                retrieved_docs = self.retriever.retrieve_with_scores(response.content, n=self.retrieve_size, score_threshold=0.4)
                input_tokens = response.usage_metadata["input_tokens"]
                output_tokens = response.usage_metadata["output_tokens"]
            for retrieved_doc in retrieved_docs:
                retrieved_doc.id = retrieved_doc.metadata.get("doc_id")
            logger.info(f"{len(retrieved_docs)} documents retrieved.")
        return {"docs_embeddings": retrieved_docs,
                "input_tokens_count": input_tokens,
                "output_tokens_count": output_tokens}

    def doc_reranker(self, state: State) -> dict:
        # GET IDS AND SCORES
        docs_embed_ids = [doc.metadata.get("doc_id") for doc in state.get("docs_embeddings", [])]
        docs_graph_ids = [doc.metadata.get("doc_id") for doc in state.get("docs_graph", [])]
        docs_reranked = []
        reranked_ids_and_scores = []
        # SKIP IF ONLY ONE METHOD
        if len(docs_embed_ids) == 0 or len(docs_graph_ids) == 0:
            logger.warning(f"At least one documents list is empty. Skipping reranking.")
        # RERANK USING IDS AND SCORES
        else:
            if state.get("reranker", "top_k") == "RRF":
                docs_reranked_ids_and_scores = self.reranker.rerank(ranked_lists=[docs_embed_ids, docs_graph_ids],
                                                               weights=[1.0, 1.0])
            else:
                reranker = TopKReranker(k=5)
                docs_embed_ids_and_scores = [(doc.metadata.get("doc_id"), doc.score) for doc in
                                             state.get("docs_embeddings", [])]
                docs_graph_ids_and_scores = [(doc.metadata.get("doc_id"), doc.score) for doc in
                                             state.get("docs_graph", [])]
                docs_reranked_ids_and_scores = reranker.rerank([docs_embed_ids_and_scores, docs_graph_ids_and_scores],
                                                                higher_better=[True, False])
            # FROM IDS TO RETRIEVED DOCUMENT --> FOR INTERNAL USAGE
            for id,_ in docs_reranked_ids_and_scores:
                doc = __get_document_from_retrieved_list__(id, state.get("docs_embeddings", []))
                if not doc:
                    doc = __get_document_from_retrieved_list__(id, state.get("docs_graph", []))
                docs_reranked.append(doc)
            # ONLY IDS AND SCORES --> THIS GOES OUT
            reranked_ids_and_scores = [RerankedDocument(id=id, score=score) for id, score in docs_reranked_ids_and_scores]
        return {"reranked_ids_and_scores": reranked_ids_and_scores,
                "docs_reranked": docs_reranked}

    def ans_generator(self, state: State) -> Command[Literal["consistency_checker", END]]:
        if state["retrieve_only"]:
            return Command(update={"answer": "",
                                   "input_tokens_count": 0,
                                   "output_tokens_count": 0,
                                   "answer_concepts": [],
                                   "status": LLMResponseStatus(status="OK")},
                           goto=END)
        else:
            logger.info(f"Generating...")
            logger.info(f"Organizing references for answer generation...")
            doc_strings = []
            # ADDITIONAL CONTEXT
            additional_context = state.get("additional_context", None)
            if type(additional_context) is str and additional_context != "":
                logger.info(f"Appending additional context...")
                doc_strings.append(f"Source [0]:\n\"{additional_context}\"")
            # DOCUMENTS
            # define a priority list: reranked, embedding, graph
            # use the highest-priority, non-empty list as reference
            prioritized_retrieved_docs_list = [state.get("docs_reranked"),
                                               state.get("docs_embeddings"),
                                               state.get("docs_graph")]
            references = []
            for retrieved_docs_list in prioritized_retrieved_docs_list:
                if len(retrieved_docs_list) > 0:
                    references = retrieved_docs_list
                    break
            references = references[0:state.get("max_refs")]
            for i, doc in enumerate(references):
                doc_strings.append(f"Source {i + 1}:\n\"{doc.page_content}\"")
            # ANSWERING
            if len(doc_strings) > 0:
                docs_content = "\n\n".join(doc_strings)
                messages = self.prompts.question_with_context_inline_cit.invoke(
                    {"question": state["query"], "context": docs_content}).messages
            else:
                messages = self.prompts.question_open.invoke({"question": state["query"]}).messages
            response = self.llm.generate(messages=messages, level="pro")
            return Command(update={"answer": response.content,
                                   "references": references,
                                   "max_refs": len(references),
                                   "input_tokens_count": response.usage_metadata["input_tokens"],
                                   "output_tokens_count": response.usage_metadata["output_tokens"]},
                           goto="consistency_checker")

    def consistency_checker(self, state: State) -> Command[Literal[END]]:
        if not state["check_consistency"]:
            logger.info(f"Skipping answer consistency...")
            return Command(
                update={"answer_concepts": [],
                        "status": LLMResponseStatus(status="OK")},
                goto=END,
            )
        else:
            logger.info(f"Checking answer consistency...")
            input_tokens = 0
            output_tokens = 0
            # QUERY CONCEPTS
            query_concepts = state["query_concepts"]
            if not query_concepts:
                query_concepts, input_tokens, output_tokens = self.__concept_extraction__(state["query"], pre_translate=state["pre_translate"])
            # ANSWER CONCEPTS
            answer_concepts, tok_in, tok_out = self.__concept_extraction__(state["answer"], pre_translate=state["pre_translate"])
            input_tokens += tok_in
            output_tokens += tok_out
            # CONSISTENCY CHECK
            qc_ids = [c.id for c in query_concepts]
            qc_names = [c.name for c in query_concepts]
            for answer_concept in answer_concepts:
                # For answer concepts that are not in the query concepts, check if there is at least one path between them and a query concept. If not, add them to the list of inconsistent concepts.
                if answer_concept.id not in qc_ids and answer_concept.name not in qc_names:
                    path_found = False
                    for question_concept_id in qc_ids:
                        if self.retriever_kg.shortest_path_bewteen(id1=answer_concept.id,
                                                                   id2=question_concept_id,
                                                                   max_hops=rag_config.get("graph").get("max_hops", 5)):
                            path_found = True
                            break
                    if not path_found:
                        answer_concept.inconsistent = True
            return Command(
                update={"answer_concepts": answer_concepts,
                        "input_tokens": input_tokens,
                        "output_tokens": output_tokens,
                        "status": LLMResponseStatus(status="OK")},
                goto=END,
            )

    def invoke(self, input_state: dict[str, Any]):
        output_state = self.graph.invoke(input_state)
        parsed_llm_output = LLMResponse(
            answer=output_state["answer"],
            consumed_tokens=ConsumedTokens(input=output_state["input_tokens_count"],
                                           output=output_state["output_tokens_count"]),
            references=References(embeddings=output_state["docs_embeddings"],
                                  graphs=output_state["docs_graph"],
                                  reranked=output_state["reranked_ids_and_scores"],
                                  used=output_state["max_refs"]),
            concepts=Concepts(query=output_state["query_concepts"],
                              answer=output_state["answer_concepts"]),
            status=output_state["status"]
        )
        return parsed_llm_output

    def get_image(self):
        try:
            return self.graph.get_graph().draw_mermaid_png()
        except Exception as e:
            logger.error(f"Error drawing graph: {e}")
            return None
